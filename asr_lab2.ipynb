{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0eabe",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель. \n",
    "\n",
    "Практическая работа разделена на 2 части: \n",
    "1. Построение нграмой языковой модели - основная часть, 10 баллов\n",
    "1. Предсказание с помощью языковой модели - дополнительная часть, 6 балла\n",
    "\n",
    "\n",
    "\n",
    "Полезные сслыки:\n",
    "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
    "* обучающие материалы - resources/lab2/lecture13_ngrams_with_SRILM.pdf\n",
    "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd5c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c1d7",
   "metadata": {},
   "source": [
    "# 1. Построение нграмной языковой модели. (10 баллов)\n",
    "\n",
    "\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле: \n",
    "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
    "\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
    "\n",
    "Поскольку униграмы не содержат в себе какого-дибо контекста, вероятность униграмы можно посчитать поделив кол-во этой слова на общее количество слов в обучающей выборке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в первую очередь нам понадобится подсчитать статистику по обучающей выборке \n",
    "def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
    "    ngrams = defaultdict(int)\n",
    "    # TODO реализуйте функцию, которая подсчитывает все 1-gram 2-gram ... order-gram ngram'ы в тексте\n",
    "    for text in train_text:\n",
    "        tokens = text.split()\n",
    "        \n",
    "        if bos:\n",
    "            tokens = ['<s>'] + tokens\n",
    "        if eos:\n",
    "            tokens = tokens + ['</s>']\n",
    "        \n",
    "        for n in range(1, order + 1):\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                ngram = tuple(tokens[i:i + n])\n",
    "                ngrams[ngram] += 1\n",
    "\n",
    "    return dict(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd69d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a passed\n"
     ]
    }
   ],
   "source": [
    "def test_count_ngrams():\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=True, eos=True) == {\n",
    "        ('<s>',): 1, \n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=True) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1,\n",
    "        ('привет', 'привет'): 1,\n",
    "        ('привет', 'как'): 1,\n",
    "        ('как', 'дела'): 1\n",
    "    }    \n",
    "    assert count_ngrams(['привет ' * 6], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 6, \n",
    "        ('привет', 'привет'): 5\n",
    "    }\n",
    "    result = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=5)\n",
    "    assert result[('<s>',)] == 3\n",
    "    assert result[('32',)] == 3\n",
    "    assert result[('<s>', 'в', 'офлайне', 'в', '32')] == 1\n",
    "    assert result[('офлайне', 'в', '32', '12', '</s>')] == 1\n",
    "    print('Test 1a passed')\n",
    "    \n",
    "    \n",
    "test_count_ngrams()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e1865",
   "metadata": {},
   "source": [
    "\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "\n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - add-k сглаживание ([Additive, Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "\n",
    "Формула сглаживания Лапласа выглядит следующим образом:\n",
    "\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
    "\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cafb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция подсчета вероятности через количество со сглаживанием Лапласа\n",
    "def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
    "    # подсчитывет ngram со сглаживанием Лапласа\n",
    "    # TODO\n",
    "    if V is None:\n",
    "        V = len([unigram for unigram in counts.keys() if len(unigram) == 1])\n",
    "    \n",
    "    if len(ngram) == 1:\n",
    "        total_unigrams = sum(count for gram, count in counts.items() if len(gram) == 1)\n",
    "        prob = (counts.get(ngram, 0) + k) / (total_unigrams + k * V)\n",
    "        return prob\n",
    "    \n",
    "    context = ngram[:-1]\n",
    "    context_count = counts.get(context, 0)\n",
    "\n",
    "    prob = (counts.get(ngram, 0) + k) / (context_count + k * V)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b25d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "def test_calculate_ngram_prob():\n",
    "    counts = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=4)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts) == 0.5\n",
    "    assert calculate_ngram_prob(('в', ), counts) == 4/25\n",
    "    assert calculate_ngram_prob(('в', ), counts, k=0.5) == (4+0.5)/(25+0.5*12)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts) == 1.0\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=1) == 0.1875\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=0) == 0.0\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=1) == 0.0625\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "\n",
    "    print(\"Test 1.b passed\")\n",
    "    \n",
    "\n",
    "test_calculate_ngram_prob()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494bf0",
   "metadata": {},
   "source": [
    "Основной метрикой язковых моделей является перплексия. \n",
    "\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd1f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Языковая модель \n",
    "class NgramLM:\n",
    "    def __init__(self, order=3, bos=True, eos=True, k=1, predefined_vocab=None):\n",
    "        self.order = order\n",
    "        self.eos = eos\n",
    "        self.bos = bos\n",
    "        self.k = k\n",
    "        self.vocab = predefined_vocab\n",
    "        self.ngrams_count = None\n",
    "        \n",
    "    @property\n",
    "    def V(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def fit(self, train_text: List[str]) -> None:\n",
    "        # TODO\n",
    "        # Подсчет vocab и ngrams_count по обучающей выборке\n",
    "        self.ngrams_count = count_ngrams(train_text, order=self.order, bos=self.bos, eos=self.eos)\n",
    "        self.vocab = [i for i in self.ngrams_count.keys() if len(i) == 1]\n",
    "                \n",
    "    \n",
    "    def predict_ngram_log_proba(self, ngram: Tuple[str]) -> float:\n",
    "        # TODO \n",
    "        # считаем логарифм вероятности конкретной нграмы\n",
    "        prob = calculate_ngram_prob(ngram, self.ngrams_count, V=self.V, k=self.k)\n",
    "        return np.log(prob)\n",
    "\n",
    "    def text_preprocess(self, text: List[str]) -> List[str]:\n",
    "        text_str = ' '.join(text)\n",
    "        # text_str = re.sub(r'[^\\w\\s-]', ' ', text_str.lower(), flags=re.UNICODE)\n",
    "        words = [word for word in text_str.split() if word and word != '-']\n",
    "        return words\n",
    "    \n",
    "    def predict_log_proba(self, words: List[str]) -> float:\n",
    "        if self.bos:\n",
    "            words = ['<s>'] + words\n",
    "        if self.eos:\n",
    "            words = words + ['</s>']\n",
    "        logprob = 0\n",
    "        # TODO \n",
    "        # применяем chain rule, чтобы посчитать логарифм вероятности всей строки\n",
    "        words = self.text_preprocess(words)\n",
    "        for i in range(len(words)):\n",
    "            if i == 0:\n",
    "                ngram = tuple(words[i:i+1])\n",
    "            else:\n",
    "                ngram = tuple(words[max(0, i-self.order+1):i+1])\n",
    "            logprob += self.predict_ngram_log_proba(ngram)\n",
    "        return logprob\n",
    "        \n",
    "    def ppl(self, text: List[str]) -> float:\n",
    "        #TODO \n",
    "        # подсчет перплексии\n",
    "        # Для того, чтобы ваш код был численно стабильным, \n",
    "        #    не считайте формулу напрямую, а воспользуйтесь переходом к логарифмам вероятностей\n",
    "        # \n",
    "        log_prob = self.predict_log_proba(text)\n",
    "        if self.bos:\n",
    "            text = ['<s>'] + text\n",
    "        if self.eos:\n",
    "            text = text + ['</s>']\n",
    "        \n",
    "        words = self.text_preprocess(text)\n",
    "        N = len(words)\n",
    "        perplexity = np.exp(-log_prob / N)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0bfe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_lm():\n",
    "    train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
    "    global lm\n",
    "    lm = NgramLM(order=2)\n",
    "    lm.fit(train_data)\n",
    "    assert lm.V == 14\n",
    "    assert np.isclose(lm.predict_log_proba(['мы']), lm.predict_log_proba([\"если\"]))\n",
    "    assert lm.predict_log_proba([\"по-моему\"]) > lm.predict_log_proba([\"если\"]) \n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14))**(-1/2)\n",
    "    ppl = lm.ppl([''])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3)\n",
    "    ppl = lm.ppl(['ЧТО'])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
    "    ppl = lm.ppl(test_data)\n",
    "    assert round(ppl, 2) == 7.33, f\"{ppl}\"\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "test_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafa0a2",
   "metadata": {},
   "source": [
    "# 2. Предсказания с помощью языковой модели (6 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7edd1f7e-74b8-473f-ad69-732f25d81bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danya/develop/ITMO_ASR_labs_2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "\n",
    "ds = load_dataset(\"zloelias/lenta-ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae284f6-9160-4212-9d47-680455256703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'text', 'topic', 'labels'],\n",
       "        num_rows: 185972\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'text', 'topic', 'labels'],\n",
       "        num_rows: 20664\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9051d3a9-0977-4e13-a877-610d4091391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ds[\"train\"][\"text\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d5d259-2eb2-4c64-941e-6040368637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_without_ = string.punctuation.replace('-', '')\n",
    "translator = str.maketrans('', '', string.punctuation)    \n",
    "\n",
    "train = [text.lower().translate(translator) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6d1ef6-ee46-4382-a828-7cda75f8ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramLM(order=2)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af97323-5ba7-4f1f-baad-5f26483988ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(lm: NgramLM, prefix, topk=4):\n",
    "    if isinstance(prefix, str):\n",
    "        prefix = prefix.split()\n",
    "    \n",
    "    candidates = []\n",
    "    for word_tuple in lm.vocab:\n",
    "        word = word_tuple[0]\n",
    "        \n",
    "        if word in ['<s>', '</s>']:\n",
    "            continue\n",
    "            \n",
    "        full_sequence = prefix + [word]\n",
    "        log_prob = lm.predict_log_proba(full_sequence)\n",
    "        \n",
    "        candidates.append((word, log_prob))\n",
    "    \n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72410ac5-9f32-48b8-8e0f-6a569052d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('года', np.float64(-29.31441989773503)), ('тура', np.float64(-30.39982732210797)), ('год', np.float64(-30.400721605144252))]\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_word(lm=model, prefix='два', topk=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe240a1-39c2-4d1e-a1fb-c4f9734d1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sentence(lm: NgramLM, prefix: str, num_words: int = 5):\n",
    "    current_sentence = prefix.split()\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        candidates = predict_next_word(lm, current_sentence, topk=1)\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "            \n",
    "        next_word = candidates[0][0]\n",
    "        current_sentence.append(next_word)\n",
    "    \n",
    "    return ' '.join(current_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7e2de2-9053-48ae-a511-5852f40eb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное: 'два'\n",
      "Дополненное: 'два года в россии'\n"
     ]
    }
   ],
   "source": [
    "prefix = \"два\"\n",
    "completed = complete_sentence(model, prefix, num_words=3)\n",
    "print(f\"Исходное: '{prefix}'\")\n",
    "print(f\"Дополненное: '{completed}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4846b",
   "metadata": {},
   "source": [
    "Попробуйте обучить ngram языковую модель на нескольких стихотворениях. Не забудьте трансформировать стихотворение в удобный для ngram модели формат (как сделать так, чтобы модель моделировала рифму?). \n",
    "Попробуйте сгенерировать продолжение для стихотворения с помощью такой языковой модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f556055-be6c-4450-8d6b-38067dc0c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Pclanglais/Onegin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69fbc55b-ae57-494d-83c9-b1be2be062ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['work', 'author', 'date', 'chapter', 'stanza', 'verse', 'text'],\n",
       "        num_rows: 977\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c179906-77d6-4706-a57f-5de01026e95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['«Мой дядя самыхъ честныхъ правилъ,', '«Когда не вшутку занемогъ,', '«Онъ уважать себя заставилъ,', '«И лучше выдумать не могъ;', '«Его примѣръ другимъ наука:'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e856ab83-26e4-455d-b179-21a0ab361784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 200 строк стихотворений\n",
      "Примеры строк:\n",
      "- мой дядя самыхъ честныхъ правилъ\n",
      "- когда не вшутку занемогъ\n",
      "- онъ уважать себя заставилъ\n",
      "- и лучше выдумать не могъ\n",
      "- его примѣръ другимъ наука\n"
     ]
    }
   ],
   "source": [
    "def prepare_poetry_data(dataset, max_lines=None):\n",
    "    poems = []\n",
    "    \n",
    "    for example in dataset['train']:\n",
    "        if max_lines and len(poems) >= max_lines:\n",
    "            break\n",
    "            \n",
    "        text = example.get('text', '')\n",
    "        if text:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line and len(line.split()) > 1:\n",
    "                    line = line.lower()\n",
    "                    poems.append(line)\n",
    "\n",
    "    punctuation_without_ = string.punctuation + '«' + '»'\n",
    "    translator = str.maketrans('', '', punctuation_without_)    \n",
    "\n",
    "    poems = [text.lower().translate(translator) for text in poems]\n",
    "    return poems\n",
    "\n",
    "poetry_data = prepare_poetry_data(ds, max_lines=200)\n",
    "\n",
    "print(f\"Загружено {len(poetry_data)} строк стихотворений\")\n",
    "print(\"Примеры строк:\")\n",
    "for i in range(min(5, len(poetry_data))):\n",
    "    print(f\"- {poetry_data[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503aa76f-c72f-4fbb-bb97-ef4ffd0970b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Четверостишие 1 ---\n",
      "Начало: 'любви надежды тихой славы'\n",
      "1: любви надежды тихой славы\n",
      "2: тихой славы забавлять забавлять\n",
      "3: забавлять забавлять забавлять забавлять\n",
      "4: забавлять забавлять забавлять забавлять\n",
      "\n",
      "--- Четверостишие 2 ---\n",
      "Начало: 'мороз и солнце день чудесный'\n",
      "1: мороз и солнце день чудесный\n",
      "2: день чудесный забавлять забавлять\n",
      "3: забавлять забавлять забавлять забавлять\n",
      "4: забавлять забавлять забавлять забавлять\n",
      "\n",
      "--- Четверостишие 3 ---\n",
      "Начало: 'я помню чудное мгновенье'\n",
      "1: я помню чудное мгновенье\n",
      "2: чудное мгновенье забавлять забавлять\n",
      "3: забавлять забавлять забавлять забавлять\n",
      "4: забавлять забавлять забавлять забавлять\n"
     ]
    }
   ],
   "source": [
    "poetry_model = NgramLM(order=2, k=0.1)\n",
    "poetry_model.fit(poetry_data)\n",
    "\n",
    "def generate_quatrain(lm: NgramLM, first_line: str):\n",
    "    lines = [first_line]\n",
    "    \n",
    "    for i in range(3):  # генерируем еще 3 строки\n",
    "        # берем последние 2 слова предыдущей строки как начало новой\n",
    "        prev_words = lines[-1].split()[-2:] if lines[-1].split() else []\n",
    "        current_line = prev_words.copy()\n",
    "        \n",
    "        while len(current_line) < 4:\n",
    "            candidates = predict_next_word(lm, current_line, topk=3)\n",
    "            if candidates:\n",
    "                current_line.append(candidates[0][0])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        lines.append(\" \".join(current_line))\n",
    "    \n",
    "    return lines\n",
    "\n",
    "# Тестирование\n",
    "start_lines = [\n",
    "    \"любви надежды тихой славы\",\n",
    "    \"мороз и солнце день чудесный\", \n",
    "    \"я помню чудное мгновенье\"\n",
    "]\n",
    "\n",
    "for i, start_line in enumerate(start_lines):\n",
    "    print(f\"\\n--- Четверостишие {i+1} ---\")\n",
    "    print(f\"Начало: '{start_line}'\")\n",
    "    \n",
    "    quatrain = generate_quatrain(poetry_model, start_line)\n",
    "    \n",
    "    for j, line in enumerate(quatrain):\n",
    "        print(f\"{j+1}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4eaf2e-9320-42ea-8794-12d0f34d4c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
